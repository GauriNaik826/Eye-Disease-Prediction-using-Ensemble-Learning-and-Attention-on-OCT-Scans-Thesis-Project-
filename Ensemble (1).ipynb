{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCHl3Ps7jh_n",
        "outputId": "4ac19d49-3520-4746-ad32-38b1794d8023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000\n",
            "500\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "image_files_list = []\n",
        "\n",
        "\n",
        "sourceImgPath = '/content/drive/MyDrive/Dataset/'\n",
        "destImgPath = '/content/drive/MyDrive/Dataset_'\n",
        "\n",
        "for root, dirs, files in os.walk(sourceImgPath):\n",
        "    for file in files:\n",
        "        if file.endswith(\".jpeg\"):\n",
        "            image_files_list.append(os.path.join(root, file))\n",
        "\n",
        "\n",
        "file_count = len(image_files_list)\n",
        "print (file_count)\n",
        "\n",
        "# print image_files_list\n",
        "filesToCopy = random.sample(image_files_list, 500)\n",
        "\n",
        "\n",
        "print(len(filesToCopy))\n",
        "\n",
        "\n",
        "# iterate over all random files and move them\n",
        "for file in filesToCopy:\n",
        "    shutil.copy(file, destImgPath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly0kV_gaB7Q5",
        "outputId": "dce7dc9e-751c-41f5-da28-64c511d64977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx9-ceftgPLl"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "data_dir = pathlib.Path('/content/drive/MyDrive/Dataset_500')\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "IMG_SIZE = 224\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjhOZPUbgUNu",
        "outputId": "42b2bf0c-3489-4bbb-8ab3-6953e3860eac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 files belonging to 4 classes.\n",
            "Using 1400 files for training.\n"
          ]
        }
      ],
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.3,\n",
        "    subset='training',\n",
        "    seed=123,\n",
        "    image_size = (img_height,img_width),batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM6iCBF-gUoC",
        "outputId": "ed348ef5-c11e-4e88-be61-3c7cd3207be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 files belonging to 4 classes.\n",
            "Using 1600 files for training.\n"
          ]
        }
      ],
      "source": [
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=123,\n",
        "    image_size = (img_height,img_width),batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtuFLejogUr6",
        "outputId": "74c0863b-2848-48fc-951e-cd66a6f2c6ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['CNV', 'DME', 'DRUSEN', 'NORMAL']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_names = train_ds.class_names\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTMkMUJggZIU"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHSDoEd-8dFE"
      },
      "source": [
        "**SELF** **ATTETNTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zIcpAd4gcxn"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.q = tf.keras.layers.Conv2D(channels // 8, 1, activation='relu', padding='same')\n",
        "        #channels // 8 expression calculates the number of output channels by dividing the channels\n",
        "        #variable by 8 using integer division. This expression is used to reduce the dimensionality of the feature maps in\n",
        "        #subsequent layers of the self-attention module.\n",
        "        self.k = tf.keras.layers.Conv2D(channels // 8, 1, activation='relu', padding='same')\n",
        "        self.v = tf.keras.layers.Conv2D(channels // 2, 1, activation='relu', padding='same')\n",
        "        self.o = tf.keras.layers.Conv2D(channels, 1, activation='relu', padding='same')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        q = self.q(inputs)\n",
        "        k = self.k(inputs)\n",
        "        v = self.v(inputs)\n",
        "\n",
        "        q = tf.reshape(q, [-1, tf.shape(q)[1] * tf.shape(q)[2], self.channels // 8])\n",
        "        k = tf.reshape(k, [-1, tf.shape(k)[1] * tf.shape(k)[2], self.channels // 8])\n",
        "        v = tf.reshape(v, [-1, tf.shape(v)[1] * tf.shape(v)[2], self.channels // 2])\n",
        "\n",
        "        attention = tf.matmul(q, tf.transpose(k, [0, 2, 1]))\n",
        "        attention = tf.nn.softmax(attention, axis=-1)\n",
        "\n",
        "        attention_out = tf.matmul(attention, v)\n",
        "        attention_out = tf.reshape(attention_out, [-1, tf.shape(inputs)[1], tf.shape(inputs)[2], self.channels // 2])\n",
        "\n",
        "        out = self.o(attention_out)\n",
        "        out = tf.keras.layers.add([out, inputs])\n",
        "\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwt4EhO88lnu"
      },
      "source": [
        "**INCEPTIONV3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5UDjmxWhDG1",
        "outputId": "d6579680-615c-4ccf-f1dc-795bc7f1dc6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.12.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 5s 0us/step\n",
            "feature_maps.shape (None, 5, 5, 2048)\n",
            "attention_maps.shape (None, 5, 5, 2048)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(tf.__version__)\n",
        "\n",
        "# load pre-trained model graph, don't add final layer\n",
        "base_model = tf.keras.applications.InceptionV3(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "                                      weights='imagenet')\n",
        "#freeze the first 10 layers\n",
        "for layer in base_model.layers[:10]:\n",
        "  layer.trainable = False\n",
        "x = base_model.output\n",
        "feature_maps = base_model.output\n",
        "print(\"feature_maps.shape\",feature_maps.shape)\n",
        "attention_maps = SelfAttention(channels=feature_maps.shape[-1])(feature_maps)\n",
        "print(\"attention_maps.shape\",attention_maps.shape)\n",
        "# add global pooling just like in InceptionV3\n",
        "new_output = tf.keras.layers.GlobalAveragePooling2D()(attention_maps)\n",
        "# add new dense layer for our labels\n",
        "#new_output = tf.keras.layers.Dense(N_CLASSES, activation='softmax')(new_output)\n",
        "new_output = tf.keras.layers.Dense(512, activation='relu')(new_output)\n",
        "new_output = tf.keras.layers.Dropout(0.4)(new_output)\n",
        "predictions = tf.keras.layers.Dense(4, activation='softmax')(new_output) # this layer used for ensemble learning\n",
        "model1 = tf.keras.Model(inputs= base_model.inputs, outputs=new_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwIKmpfTIkb9",
        "outputId": "147619f5-4dc0-4e82-e375-42c5266373e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.0166 - accuracy: 0.2043\n",
            "Epoch 1: saving model to /content/drive/MyDrive/capstone/model1/model_01-0.2043.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 126s 2s/step - loss: 9.0166 - accuracy: 0.2043 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 2/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.1842 - accuracy: 0.2050\n",
            "Epoch 2: saving model to /content/drive/MyDrive/capstone/model1/model_02-0.2050.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 54s 1s/step - loss: 9.1842 - accuracy: 0.2050 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 3/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.6585 - accuracy: 0.1793\n",
            "Epoch 3: saving model to /content/drive/MyDrive/capstone/model1/model_03-0.1793.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 53s 1s/step - loss: 9.6585 - accuracy: 0.1793 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 4/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.3493 - accuracy: 0.1950\n",
            "Epoch 4: saving model to /content/drive/MyDrive/capstone/model1/model_04-0.1950.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 54s 1s/step - loss: 9.3493 - accuracy: 0.1950 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 5/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.1454 - accuracy: 0.1936\n",
            "Epoch 5: saving model to /content/drive/MyDrive/capstone/model1/model_05-0.1936.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 59s 1s/step - loss: 9.1454 - accuracy: 0.1936 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 6/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.1891 - accuracy: 0.1929\n",
            "Epoch 6: saving model to /content/drive/MyDrive/capstone/model1/model_06-0.1929.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 63s 1s/step - loss: 9.1891 - accuracy: 0.1929 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 7/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.3119 - accuracy: 0.1843\n",
            "Epoch 7: saving model to /content/drive/MyDrive/capstone/model1/model_07-0.1843.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 62s 1s/step - loss: 9.3119 - accuracy: 0.1843 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 8/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.2959 - accuracy: 0.1964\n",
            "Epoch 8: saving model to /content/drive/MyDrive/capstone/model1/model_08-0.1964.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 51s 1s/step - loss: 9.2959 - accuracy: 0.1964 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 9/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.5063 - accuracy: 0.1843\n",
            "Epoch 9: saving model to /content/drive/MyDrive/capstone/model1/model_09-0.1843.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 57s 1s/step - loss: 9.5063 - accuracy: 0.1843 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 10/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.2583 - accuracy: 0.2064\n",
            "Epoch 10: saving model to /content/drive/MyDrive/capstone/model1/model_10-0.2064.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 56s 1s/step - loss: 9.2583 - accuracy: 0.2064 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 11/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.3185 - accuracy: 0.1864\n",
            "Epoch 11: saving model to /content/drive/MyDrive/capstone/model1/model_11-0.1864.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 61s 1s/step - loss: 9.3185 - accuracy: 0.1864 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 12/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.2389 - accuracy: 0.2029\n",
            "Epoch 12: saving model to /content/drive/MyDrive/capstone/model1/model_12-0.2029.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 61s 1s/step - loss: 9.2389 - accuracy: 0.2029 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 13/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.1090 - accuracy: 0.1964\n",
            "Epoch 13: saving model to /content/drive/MyDrive/capstone/model1/model_13-0.1964.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 60s 1s/step - loss: 9.1090 - accuracy: 0.1964 - val_loss: 5.4158 - val_accuracy: 0.2488\n",
            "Epoch 14/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.3852 - accuracy: 0.1950\n",
            "Epoch 14: saving model to /content/drive/MyDrive/capstone/model1/model_14-0.1950.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 106). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r44/44 [==============================] - 61s 1s/step - loss: 9.3852 - accuracy: 0.1950 - val_loss: 5.4158 - val_accuracy: 0.2488\n"
          ]
        }
      ],
      "source": [
        "model_filepath = '/content/drive/MyDrive/capstone/model1/model_{epoch:02d}-{accuracy:.4f}.tf'\n",
        "\n",
        "model1.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-2),\n",
        "              metrics=['accuracy'])\n",
        "checkpoint1 = tf.keras.callbacks.ModelCheckpoint(filepath=model_filepath,\n",
        "                                                monitor='val_accuracy',mode='max',save_freq='epoch',\n",
        "                                                verbose=1)\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history1= model1.fit(train_ds, validation_data=val_ds, epochs=25, callbacks=[stop_early,checkpoint1])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh93WrTT8uF4"
      },
      "source": [
        "**Xception**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uASx2CcCh-Ie",
        "outputId": "5ff47a47-e2ea-4b5b-b486-1c9c5094002b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n",
            "feature_maps.shape (None, 7, 7, 2048)\n",
            "attention_maps.shape (None, 7, 7, 2048)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# load pre-trained model graph, don't add final layer\n",
        "base_model = tf.keras.applications.Xception(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "                                      weights='imagenet')\n",
        "#freeze the first 10 layers\n",
        "for layer in base_model.layers[:10]:\n",
        "  layer.trainable = False #not training base model only training top layers so not traiing from scratch\n",
        "feature_maps = base_model.output\n",
        "print(\"feature_maps.shape\",feature_maps.shape)\n",
        "attention_maps = SelfAttention(channels=feature_maps.shape[-1])(feature_maps)\n",
        "print(\"attention_maps.shape\",attention_maps.shape)\n",
        "# add global pooling just like in InceptionV3\n",
        "new_output = tf.keras.layers.GlobalAveragePooling2D()(attention_maps)\n",
        "# add new dense layer for our labels\n",
        "#new_output = tf.keras.layers.Dense(N_CLASSES, activation='softmax')(new_output)\n",
        "new_output = tf.keras.layers.Dense(512, activation='relu')(new_output)\n",
        "new_output = tf.keras.layers.Dropout(0.3)(new_output)\n",
        "predictions = tf.keras.layers.Dense(4, activation='softmax')(new_output) # this layer used for ensemble learning\n",
        "model3 = tf.keras.Model(inputs= base_model.inputs, outputs=new_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oceTWy269Shs",
        "outputId": "1456c8ec-701f-4d8a-953e-3fed051bc1fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 10.0856 - accuracy: 0.2514 \n",
            "Epoch 1: saving model to /content/drive/MyDrive/capstone/model3/model_01-0.2514.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 66). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44/44 [==============================] - 1576s 35s/step - loss: 10.0856 - accuracy: 0.2514 - val_loss: 8.7321 - val_accuracy: 0.2475\n",
            "Epoch 2/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.6226 - accuracy: 0.2564 \n",
            "Epoch 2: saving model to /content/drive/MyDrive/capstone/model3/model_02-0.2564.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 66). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44/44 [==============================] - 1548s 35s/step - loss: 9.6226 - accuracy: 0.2564 - val_loss: 8.7321 - val_accuracy: 0.2475\n",
            "Epoch 3/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.6979 - accuracy: 0.2536 \n",
            "Epoch 3: saving model to /content/drive/MyDrive/capstone/model3/model_03-0.2536.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 66). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44/44 [==============================] - 1469s 34s/step - loss: 9.6979 - accuracy: 0.2536 - val_loss: 8.7321 - val_accuracy: 0.2475\n",
            "Epoch 4/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.8874 - accuracy: 0.2443 \n",
            "Epoch 4: saving model to /content/drive/MyDrive/capstone/model3/model_04-0.2443.tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 66). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44/44 [==============================] - 1436s 33s/step - loss: 9.8874 - accuracy: 0.2443 - val_loss: 8.7321 - val_accuracy: 0.2475\n",
            "Epoch 5/25\n",
            "44/44 [==============================] - ETA: 0s - loss: 9.9410 - accuracy: 0.2507 "
          ]
        }
      ],
      "source": [
        "model_filepath = '/content/drive/MyDrive/capstone/model3/model_{epoch:02d}-{accuracy:.4f}.tf'\n",
        "\n",
        "model3.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-2),\n",
        "              metrics=['accuracy'])\n",
        "checkpoint3 = tf.keras.callbacks.ModelCheckpoint(filepath=model_filepath,\n",
        "                                                monitor='val_accuracy',mode='max',save_freq='epoch',\n",
        "                                                verbose=1)\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history3= model3.fit(train_ds, validation_data=val_ds, epochs=25, callbacks=[stop_early,checkpoint3])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ka_lzIK92Um"
      },
      "source": [
        "**LOADING THE TRAINED MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIeCKET98OVE"
      },
      "outputs": [],
      "source": [
        "model_1= tf.keras.models.load_model('/content/drive/MyDrive/capstone/model1/model_10-0.2064.tf')\n",
        "model_1= tf.keras.models.Model(inputs = model_1.inputs, outputs = model_1.outputs, name ='name_of_model1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPeTH3N-98g9"
      },
      "outputs": [],
      "source": [
        "model_2= tf.keras.models.load_model('/content/drive/MyDrive/capstone/model2/-01-0.0000.tf')\n",
        "model_2= tf.keras.models.Model(inputs = model_2.inputs, outputs = model_2.outputs, name ='name_of_model2')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOusjlXp-LgV"
      },
      "source": [
        "**TRAIN ENSEMBLE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEwzyzoQjEpt"
      },
      "outputs": [],
      "source": [
        "model_filepath = '/content/drive/MyDrive/capstone/ensemble_model/-{epoch:02d}-{val_accuracy:.4f}.tf'\n",
        "checkpoint_final = tf.keras.callbacks.ModelCheckpoint( filepath=model_filepath,\n",
        "                                                monitor='val_accuracy',mode='max',save_best_only=True,\n",
        "                                                verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzBsly4VjLVU"
      },
      "outputs": [],
      "source": [
        "models = [model_1, model_2]\n",
        "model_input = tf.keras.layers.Input(shape=(224,224,3))\n",
        "model_outputs = [model(model_input) for model in models]\n",
        "ensemble_output = tf.keras.layers.Average()(model_outputs)\n",
        "ensemble_model = tf.keras.models.Model(inputs = model_input, outputs= ensemble_output, name='ensemble')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6116SlW-Wty"
      },
      "outputs": [],
      "source": [
        "model_filepath = '/content/drive/MyDrive/capstone/ENSEMBLE/model_{epoch:02d}-{accuracy:.4f}.tf'\n",
        "\n",
        "ensemble_model.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-2),\n",
        "              metrics=['accuracy'])\n",
        "checkpoint_ = tf.keras.callbacks.ModelCheckpoint(filepath=model_filepath,\n",
        "                                                monitor='val_accuracy',mode='max',save_freq='epoch',\n",
        "                                                verbose=1)\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history_= ensemble_model.fit(train_ds, validation_data=val_ds, epochs=25, callbacks=[stop_early,checkpoint_])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eohf7b9NjVyX"
      },
      "outputs": [],
      "source": [
        "acc = history_.history['accuracy']\n",
        "val_acc = history_.history['val_accuracy']\n",
        "loss = history_.history['loss']\n",
        "val_loss = history_.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'b',label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r',label='Validation accuracy')\n",
        "plt.title('Training and Validation Acuuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b',label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r',label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.figure()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}